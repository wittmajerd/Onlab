{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.datasets import BiosensorDataset\n",
    "from src.unet import UNet\n",
    "from src.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "data_path = 'data_with_centers/'\n",
    "train_percent = 0.86\n",
    "bio_len = 16\n",
    "mask_size = 80\n",
    "batch_size = 4\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "train_size = int(train_percent * len(files))\n",
    "val_size = len(files) - train_size\n",
    "train_files, val_files = torch.utils.data.random_split(files, [train_size, val_size])\n",
    "\n",
    "mean, std = calculate_mean_and_std(data_path, train_files, biosensor_length=bio_len)\n",
    "\n",
    "train_dataset = BiosensorDataset(data_path, train_files, mean, std, bool, biosensor_length=bio_len, mask_size=mask_size)\n",
    "val_dataset = BiosensorDataset(data_path, train_files, mean, std, bool, biosensor_length=bio_len, mask_size=mask_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_lightning import UNetLightningModule, BiosensorDataModule\n",
    "from src.losses import DiceLoss, IoULoss\n",
    "\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Define the hyperparameters\n",
    "args = Namespace(\n",
    "    lr=0.001,\n",
    "    epochs=10,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    amp=False,\n",
    "    bilinear=False,\n",
    "    data_path='data_with_centers/'\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = DiceLoss()\n",
    "\n",
    "# Initialize the model and data module\n",
    "model = UNetLightningModule(learning_rate=args.lr, channels=BIO_LENGTH, classes=1, loss_func=criterion, amp=args.amp, bilinear=args.bilinear)\n",
    "data_module = BiosensorDataModule(data_path=args.data_path, batch_size=args.batch_size, biosensor_length=BIO_LENGTH, mask_size=MASK_SIZE)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(max_epochs=args.epochs, accelerator='gpu' if torch.cuda.is_available() else 'cpu', precision=16 if args.amp else 32)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data_with_centers/'\n",
    "\n",
    "dataset = BiosensorDataset(data_path, mask_type=bool, biosensor_length=BIO_LENGTH, mask_size=80)\n",
    "\n",
    "TRAIN_SIZE = int(len(dataset)*0.86)\n",
    "VAL_SIZE = len(dataset) - TRAIN_SIZE\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [TRAIN_SIZE, VAL_SIZE])\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = UNet(n_channels=BIO_LENGTH, n_classes=1)\n",
    "model = model.to(device)\n",
    "\n",
    "try:\n",
    "    train_model(\n",
    "        model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        learning_rate=0.01,\n",
    "        epochs=10,\n",
    "        # checkpoint_dir=checkpoint_dir,\n",
    "        amp=True,\n",
    "    )\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    torch.cuda.empty_cache()\n",
    "    print('Detected OutOfMemoryError!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
